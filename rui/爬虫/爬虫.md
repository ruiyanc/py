##Robots协议
* 爬虫协议,网站根目录下的robots.txt文件,指定是否可爬

##urllib
* urllib.request模块
    * urlopen():http请求，返回HTTPResponse对象
        * read():返回网页内容
        * status:结果的状态码
    * Request(url,[参数,headers,ip地址,False,method]):详细请求
    * HTTPBasicAuthHandler:设置验证值
*     * p = HTTPPasswordMarWithDefaultRealm() 
        auth_header = HTTPBasicAuthHandler(p.add_password(None,username,password))
        html = build_opener(auth_header).open(url).read().decode('utf-8')
    * 代理:build_opener(ProxyHandler({'http':'127...'})).open(url)
    * Cookies:
*     * cookie = http.cookiejar.CookieJar()
        handler = urllib.request.HTTPCookieProcessor(cookie)
        response = urllib.request.build_opener(handler).open(url)
* urllib.parse模块
    * urlparse(),urlsplit():url解析
    * urljoin():url拼接合并
    * `urlencode()`:字典数据序列化为GET请求参数
    * parse_qs(),parse_qsl():反序列化为字典,元组列表
    * quote(),unquote():中文字符编码,解码
* urllib.robotparser模块
    * RobotFileParser('http://..../robots.txt')
        * read():读取robots.txt文件并进行分析
        * can_fetch():判断网页是否可抓取
        
##request
* request.get(url,[params,headers]):GET请求
* request.post(url,data,[files])
    * text:字符类型,content:字节类型,json():JSON格式,
    * headers:响应头,status_code:状态码,timeout=(连接,读取):超时时间
    * Cookies:获取Cookies
    * 文件上传:`files={'file':open('...jpg','rb')}`
    * Session:`requests.Session()`
    * 身份认证:auth=('username', 'password')
    
##正则表达式 (re)
* 匹配规则
    * \w:字母数字下划线
    * \s:任意空白字符<=>[\t\n\r\f]
    * \d:任意数字<=>[0-9]
    * ^:开头,$:结尾
    * \[...]:一组字符中一个
    * .:任意字符; *:0或n; +:1或n; ?:0或１,非贪婪
    * {n,m}:n到m次,贪婪; {n}:n个
    * a|b:a或b; ():括号内的,或一个组
* match([re.S]):从字符串的起始位置匹配,(全部)要考虑开头的内容
* search():依次扫描字符串,返回第一个符合条件的匹配目标,(部分)
* findall():搜索整个字符串,返回匹配的所有内容
* sub(匹配的,替换的):修改文本
* compile():将正则字符串编译成正则表达式对象,便于复用
    * `re.I:无视大小写;re.S:.加强匹配所有;`re.M:多行匹配
    * group():输出匹配到的内容
    * span():输出匹配的范围
#解析库
##XPath:xml路径语言
* xpath常用规则:
    * nodename:选取此节点的所有子节点
    * /:从当前节点选取直接子节点
    * //:从当前节点选取子孙节点
    * .:选取当前节点
    * ..:选取当前节点的父节点
    * @:选择属性; *:匹配所有节点
    * xpath(//text()):获取子孙节点内部的所有文本
    * contains():多值匹配
* lxml.etree模块
    * HTML():初始化为xpath解析对象
    * tostring():输出修正后的html代码,bytes类型
    * `parse('xx', HTMLParse())`:直接文本解析,再xpath匹配
##Beautiful Soup
* soup=BeautifulSoup(html, 'lxml'):获取BeautifulSoup对象
* prettify():将要解析的字符串以标准格式输出
* soup.节点名:获取第一个该节点的所有内容
    * name:节点名称; attrs['class']:获取属性;string:文本内容
    * contents:直接子节点的列表; children:直接子节点的生成器
    * descendants:所有的子孙节点
    * parent:父节点; parents:祖先节点
    * next_sibling:下一个兄弟节点; previous_sibling:上一个
* soup.find_all(name,attrs,text):查询所有符合条件的元素的列表
* soup.find():第一个匹配的元素
    * name:节点名;attrs={}:属性
    * text:用来匹配节点的文本,可字符串可正则对象    
* css选择器`select()`:通过css查询

##pyquery(css)
* 初始化:pq=pyquery.PyQuery(html[url,filename])
    * css选择:pq(#box .list li)
        * attr():获取属性
        * text():获取其内部文本
        * html():获取节点内部的HTML文本
        * remove():移除
    * find():所有符合条件的子孙节点; children():符合的子节点
    * parent():获取当前节点的父节点; parents():祖先节点
    * sibling():兄弟节点
        * items():获取多节点的结果集,可迭代获取
   
#数据存储
## 文本存储
* txt文本存储
    * with open():文件操作
        * a:追加;b:二进制;r:只读;w:写入
* JSON文件存储:任何支持的类型都可以
    * `JSON数据需要用双引号包围`,不能使用单引号
    * JSON库
        * loads():将json文本字符串转为json对象(读取本地)
            * get():传入键名获取值
        * dumps(data,[indent,ensure_ascii]):将json对象转为文本字符串(写入到本地)
            * indent:代表缩进字符个数
            * ensure_ascii=False:输出中文
* CSV文件存储:字符序列,分隔符常用:逗号和制表符
    * writer():初始化写入对象
    * `DictWriter()`:初始化字典写入对象
        * writerow():传入每行数据完成写入;writerows():多行写入
    * pandas库的DataFrame对象的to_csv():将数据写入csv文件
    * reader():读取文件内容
    * pandas.read_csv():将数据从csv中读取出来
##数据库存储
* MongoDB
    * client=pymongo.MongoClient('mongodb://localhost:27017/'):连接
    * db=client.库名:指定数据库
    * collection=db.集合名:指定集合
        * insert_one():插入单条记录;insert_many():插入多条
        * find_one():查询单条;find():查询多条,返回生成器对象
            * 传参为字典键值对查询
                * 例:results=collection.find({'age':{'$gt':20}}):查询age大于20
                * $regex:指定正则匹配
                * $where:高级条件查询
            * count():计数
            * sort('age',pymongo.ASCENDING):升序排序
                * skip():偏移几个位置
        * update_one(),update_many():更新数据
            * '$set':'查询的'
            * matched_count:匹配的数据条数;modified_count:影响的数据条数
        * delete_one():删除第一条符合条件的数据;delete_many():删除所有符合
            * delete_count:获取删除的数据条数

##Ajax数据爬取
* Ajax请求(Type:xhr):`X-Requested-With:XMLHttpRequest`
* cardlistInfo和cards
    * total:总数量
    
##Selenium
* `browser=webdriver.Chrome()`:初始化浏览器对象并赋值为browser对象
    * page_source:获取网页的源码
    * current_url():当前url;get_cookies():当前cookies
* get(url):请求网页
* find_element_by_?():获取单个节点
    * id:根据id获取;name:根据name值获取
    * xpath:根据xpath获取
    * css_selector:css选择器获取
* 通用获取方法:find_element(By.ID,id)==find_element_by_id(id)
* find_elements():获取多个节点
* 模拟节点交互:
    * send_keys():模拟输入文字
    * clear():清空文字
    * click():点击按钮
* 动作链(鼠标拖拽等):
    * 声明ActionsChains对象
        * drag_and_drop(起始节点,目标节点):拖拽操作
        * perform():执行动作
* 执行JavaScript
    * execute_script():模拟运行JavaScript
* 获取节点信息
    * get_attribute(属性名):获取节点的属性
    * text:获取文本值;id:获取节点id
    * location:该节点在页面的相对位置
    * tag_name:获取标签名称;size:获取节点大小(宽高)
* 切换Frame(切换父子框架)
    * switch_to.frame():切换Frame
* 延时等待 
    1. 隐式等待:查找节点而节点没有立即出现时,等待一段时间再查找DOM
        * implicitly_wait():等待时间
    2. 显式等待:规定时间加载,有则返回查找的节点,无则超时异常
        * wait=WebDriverWait(browser,时间):引入WebDriverWait对象
            * 调用until()传入要等待条件expected_conditions
                * http://selenium-python.readthedocs.io/api.html
                * presence_of_element:节点加载出现
                * title_contains:标题包含某内容
                * visibility_of:可见
                * frame_to_be_available_and_switch_to_it:加载并切换
                * text_to_be_present_in_element:某节点文本包含某文字
* 前进和后退
    * back():后退;forward():前进
* Cookies
    * get_cookies():获取
    * add_cookie():添加
    * delete_all_cookies():删除
* 选项卡管理
    * execute_script('window.open()):开启一个选项卡
    * window.handles:获取当前开启的所有选项卡
    * switch_to_window(browser.window_handles[1]):切换选项卡
    
##Splash
* 带有http的轻量级浏览器,通过lua脚本执行渲染操作

##验证码的识别
* 图形验证码:tesserocr库
    * 本地化环境处理:
    *       import locale
            locale.setlocale(locale.LC_ALL, 'C')
    * 新建Image对象:Image.open()
        * image_to_text():识别Image对象
        * convert('L'):将图片转化为灰度图像;('1'):二值化处理
* 滑动验证码的识别

##代理
* requests:`proxies`
* webdriver.Chrome(chrome_options.add_argument('--proxy-server=http://'))
* 代理池
    * 
    
 